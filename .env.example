# Codenames RL Configuration
# Copy this file to .env and modify as needed

# ============================================================================
# Game Environment Settings
# ============================================================================
MAX_TURNS=20
MAX_GUESSES=9

# ============================================================================
# Evaluation Settings
# ============================================================================
NUM_GAMES=100
START_SEED=0
AGENT_SEED=42

# ============================================================================
# File Paths
# ============================================================================
# Paths can be relative to project root or absolute
WORDLIST_PATH=configs/wordlist_en.txt
VOCABULARY_PATH=configs/vocabulary_en.txt

# ============================================================================
# Model Settings
# ============================================================================
# Embedding model for semantic similarity
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Legacy LLM model path (deprecated, use LLM_MODEL_NAME instead)
LLM_MODEL_PATH=

# ============================================================================
# LLM Model Configuration
# ============================================================================
# HuggingFace model identifier or local path
# Examples:
#   - Qwen/Qwen2.5-7B-Instruct (default, 7B model)
#   - Qwen/Qwen2.5-14B-Instruct (14B model)
#   - Qwen/Qwen2.5-32B-Instruct (32B model)
#   - google/gemma-2b-it (Gemma 2B Instruct)
#   - google/gemma-7b-it (Gemma 7B Instruct)
#   - google/gemma-9b-it (Gemma 9B Instruct)
#   - Qwen/Qwen2.5-20B-Instruct (Qwen 20B - can use with 4-bit quantization)
#   - openchat/openchat-3.5-1210 (OpenChat 3.5)
LLM_MODEL_NAME=Qwen/Qwen2.5-7B-Instruct

# LLM generation temperature (0.0 = deterministic, 1.0+ = more creative)
LLM_TEMPERATURE=0.7

# Maximum number of tokens to generate
LLM_MAX_NEW_TOKENS=128

# Quantization mode for memory-efficient model loading
# Options: none, 4bit, 8bit
# - "none": No quantization (full precision, requires most memory)
# - "4bit": 4-bit quantization (~4x memory reduction, CUDA only)
# - "8bit": 8-bit quantization (~2x memory reduction, CUDA only)
# 
# Example: Use 4-bit quantization for large models like 20B
# LLM_QUANTIZATION=4bit
# LLM_MODEL_NAME=Qwen/Qwen2.5-20B-Instruct
LLM_QUANTIZATION=none

# ============================================================================
# Performance Settings
# ============================================================================
# Device to use: "cpu", "cuda", "mps" (Apple Silicon)
# Leave empty for auto-detection
DEVICE=

# Batch size for embedding operations
EMBEDDING_BATCH_SIZE=32
